## Get hostname with the following command: 
## $ hostname -f
##
## Configure environment per hostname:
## [hostname1]
## ...
##
## Use the same environment for multiple hostnames:
## [hostname2, hostname3, ...]
## ...
##
## Using group
## [hostname1, hostname2, ... : group]
## [group]
## ...
##
## Using an asterisk in hostnames (IMPORTANT: only one * is allowed in hostnames)
##
## [host*name1]
##
## [*hostname2, hostname3*]


# Stanford Kundaje group clusters (out of SGE)
[vayu, mitra, durga]
conda_env	= bds_atac
conda_env_py3	= bds_atac_py3
species_file	= $script_dir/species/kundaje.conf
unlimited_mem_wt= true 		# unlimited max. memory and walltime on Kundaje clusters
nice 		= 5
nth 		= 3

# Stanford Kundaje group clusters (controlled with SGE)
[nandi, kali, amold, wotan, kadru, surya]
conda_env	= bds_atac
conda_env_py3	= bds_atac_py3
species_file	= $script_dir/species/kundaje.conf
unlimited_mem_wt= true 		# unlimited max. memory and walltime on Kundaje clusters
system 		= sge
nice 		= 5
nth 		= 3

# Stanford SCG4
[scg*.stanford.edu, scg*.local, carmack.stanford.edu, crick.stanford.edu]
conda_env	= bds_atac
conda_env_py3	= bds_atac_py3
species_file	= $script_dir/species/scg.conf
nth		= 4		# number of threads for each pipeline
system		= sge		# force to use SGE (Sun Grid Engine) on SCG3/4 even though a user doesn't explicitly specify SGE on command line with 'bds -s sge atac.bds ...'


# Stanford Sherlock clusters
[sherlock*.stanford.edu, sh-*.local]
conda_env	= bds_atac
conda_env_py3	= bds_atac_py3
species_file	= $script_dir/species/sherlock.conf
nth		= 4		# number of threads for each pipeline
system		= slurm


# default
[default]
conda_env	= bds_atac
conda_env_py3	= bds_atac_py3
species_file	= /usit/abel/u1/ivargry/ivargry/skrot/dnase_seq_data/bds_atac_species.conf	# DEF_SPECIES_FILE: DO NOT MODIFY THIS COMMENT (install_genome_data.sh WILL NOT WORK).

